{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USTRqjLDr-vI"
   },
   "source": [
    "# Parameter reconstruction for non-linear stress-strain relations\n",
    "\n",
    "## Problem statement\n",
    "The purpose of this project is to implement various machine learning techniques to reconstruct the model parameters from given stress-strain curves. It is a priori known that the stress-strain relation is given by an explicit *Ramberg-Osgood* model, which represents a non-linear relation between the stress $\\sigma$ and the strain $\\varepsilon$, namely\n",
    "$$\n",
    "\\sigma = C \\, \\varepsilon^{1/n}\\,.\n",
    "$$\n",
    "Herein, $C$ and $n$ denote the material parameters, which should be identified based on given stress-strain curves. \n",
    " \n",
    "## Data-driven approach\n",
    "The implemented machine learning models learn the non-linear stress-strain relation through training data. Herein, each training sample corresponds to vector-valued stresses and strains, which are mapped to the corresponding model parameters. To this end, the following split is proposed:\n",
    "\n",
    "*   features (vector-valued): $\\varepsilon$, $\\sigma$\n",
    "*   labels: $C$, $n$\n",
    "\n",
    "Note that the labels represent the target of a prediction, while the data generation process will be explained later. Since two parameters correspond to each stress-strain curve, the machine learning problem is consequently classified as a **multi-target regression**.\n",
    "\n",
    "\n",
    "## Model requirements\n",
    "The trained model should predict the material parameters from stress-strain curves, while the following requirements should be addressed through the implementation:\n",
    "\n",
    "* noise in the stress-strain curves to represent experimental data,\n",
    "* data points for the strain values are unevenly distributed,\n",
    "* flexible adjustment of the number of data points in stress-strain curves.\n",
    "\n",
    "## Solution procedure\n",
    "\n",
    "* Analyze an exemplary stress-strain curve.\n",
    "* Generating apropriate test and train data.\n",
    "* Implement a machine learning model.\n",
    "* Training the model and optimize hyperparameter.\n",
    "* Evaluate the generalization performance on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpFxqxDgLNIA"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ZWoL_o3KCLpv",
    "outputId": "ce452144-c036-4f63-9752-71799b83ec58"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4McOJb56Ua_7"
   },
   "source": [
    "## Custom plot style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZv_T0F0Dwk6"
   },
   "outputs": [],
   "source": [
    "# set custom matplotlib rc file \n",
    "url_rcfile = 'https://raw.githubusercontent.com/TheFerry10/ml-parameter-identification/master/seaborn-custom-style.mplstyle'\n",
    "#path_rcfile = 'seaborn-custom-style.mplstyle'\n",
    "mpl.style.use(url_rcfile)\n",
    "\n",
    "# Uncomment the following lines, if you wish to use latex fonts in matplotlib\n",
    "# figures. The first line installs latex fonts, while the second line activates\n",
    "# the fonts in the matplotlib rc file\n",
    "\n",
    "#!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng\n",
    "mpl.rcParams['text.usetex'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ-y-ESnyDpm"
   },
   "outputs": [],
   "source": [
    "class CustomPlotStyle(object):\n",
    "    def __init__(self, textwidth_in_inch=6.30):\n",
    "        self.ASPECT_RATIO = 4/3\n",
    "        self.TEXTWIDTH_LATEX = textwidth_in_inch\n",
    "        self.SCALE = 1.0\n",
    "        self.WIDTH = self.SCALE * self.TEXTWIDTH_LATEX\n",
    "        self.HEIGHT = self.WIDTH / self.ASPECT_RATIO\n",
    "        self.PALETTE = sns.color_palette(mpl.colors.TABLEAU_COLORS)\n",
    "        self.MARKERS = ['o', 's', '^', 'v', 'D','P','X','*']\n",
    "\n",
    "    def create_figure(self, width=None, height=None, aspect_ratio=None, width_scale=None):\n",
    "        if width_scale is None:\n",
    "            width_scale = self.SCALE\n",
    "        if width is None:\n",
    "            width = self.TEXTWIDTH_LATEX * width_scale\n",
    "        if height is None:\n",
    "            height = width / self.ASPECT_RATIO\n",
    "        if aspect_ratio is not None:\n",
    "            height =  width / aspect_ratio\n",
    "        fig, ax = plt.subplots(figsize=(width, height), tight_layout=True)\n",
    "        return fig, ax    \n",
    "    \n",
    "    def get_marker_cycle(self):\n",
    "        return itertools.cycle(self.MARKERS)\n",
    "\n",
    "    def get_color_cycle(self):\n",
    "        return itertools.cycle(self.PALETTE)  \n",
    "\n",
    "\n",
    "def create_label_without_line(label, ax):\n",
    "    return ax.plot([],[],label=label, linewidth=0)\n",
    "\n",
    "def set_figsize(width=6.3, aspect_ratio=4/3, width_scale=1.0):\n",
    "    width = width * width_scale\n",
    "    height =  width / aspect_ratio\n",
    "    return (width, height)    \n",
    "\n",
    "def save_fig(file_name, output_dir='', dpi=300, file_extensions=['.pdf','.png']):\n",
    "    file_name = os.path.join(output_dir,file_name)\n",
    "    for file_extension in file_extensions:\n",
    "        plt.gcf().savefig(file_name + file_extension,dpi=dpi)\n",
    "        \n",
    "        \n",
    "class PlotLabels(object):\n",
    "    \"\"\"Creating plot labels for convenience and use them globally\"\"\"\n",
    "    def __init__(self):\n",
    "        self.time = r'Time $t$ [s]'\n",
    "        self.stress = r'Stress $\\sigma$ [N/m$^2$]'\n",
    "        self.strain = r'Strain $\\varepsilon$ [-]'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSPNn9OkzgOs"
   },
   "outputs": [],
   "source": [
    "plotStyle = CustomPlotStyle(textwidth_in_inch=10.0)\n",
    "plotLabels = PlotLabels()\n",
    "sns.set_context(\"talk\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZURFmnv2MQy"
   },
   "outputs": [],
   "source": [
    "PATH_TO_FIGURES = 'fig/'\n",
    "if not os.path.isdir(PATH_TO_FIGURES):\n",
    "    os.mkdir(PATH_TO_FIGURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_EsLpxSMLXv"
   },
   "source": [
    " # Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjF6CsqBWX49"
   },
   "source": [
    "First, an exemplary stress-strain curve is loaded and the number of datapoints is extracted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D88MViYlSXQJ"
   },
   "outputs": [],
   "source": [
    "url_dataset = 'https://raw.githubusercontent.com/TheFerry10/ml-parameter-identification/master/example-stress-strain-curve.csv'\n",
    "#path_dataset = 'example-stress-strain-curve.csv'\n",
    "df_stress_strain_curve = pd.read_csv(url_dataset)\n",
    "NUM_DATAPOINTS, NUM_VARS  = df_stress_strain_curve.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uef8mSvXc_G"
   },
   "source": [
    "The rolling mean and standard deviation corresponding corresponding to a fixed window size are calculated from the data. The averaged standard deviation will be later used to generate stress-strain curves, which obtain similar signal-to-noise values in the stress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7SfQf3-bB6i2",
    "outputId": "e63d7d50-4255-4018-a6ca-78e2cf0aa087"
   },
   "outputs": [],
   "source": [
    "window = 4\n",
    "num_std = 1\n",
    "df_stress_strain_curve[\"rollingMean\"] = df_stress_strain_curve[\"sig\"].rolling(window).mean()\n",
    "df_stress_strain_curve[\"rollingStd\"] = df_stress_strain_curve[\"sig\"].rolling(window).std()\n",
    "std_estimated_from_stress_strain_curve = df_stress_strain_curve[\"rollingStd\"].mean()\n",
    "print(\"Estimated averaged standard deviation:\", std_estimated_from_stress_strain_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw9UIDSnaNeb"
   },
   "source": [
    "The stress-strain curve is visualized together with the rolling mean and error bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8c0_Vi3alMG"
   },
   "outputs": [],
   "source": [
    "df_stress_strain_curve[\"lowErrorBand\"] = df_stress_strain_curve[\"rollingMean\"] - num_std * std_estimated_from_stress_strain_curve\n",
    "df_stress_strain_curve[\"highErrorBand\"] = df_stress_strain_curve[\"rollingMean\"] + num_std * std_estimated_from_stress_strain_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "_CkRIL9VCn6_",
    "outputId": "0b045981-58bf-4d0a-9314-5badfdbc91c6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plotStyle.create_figure()\n",
    "plt.plot(\"eps\", \"sig\", data=df_stress_strain_curve, label='raw data', marker='o', linewidth=0)\n",
    "plt.plot(\"eps\", \"rollingMean\", data=df_stress_strain_curve, label=f'rolling mean over {window} datapoints')\n",
    "plt.fill_between(x=\"eps\", y1=\"highErrorBand\", y2=\"lowErrorBand\", data=df_stress_strain_curve, alpha=0.2,\n",
    "                 label='error band ($\\sigma_{std}$' + ' = {:.2f})'.format(std_estimated_from_stress_strain_curve))\n",
    "plt.xlabel(plotLabels.strain)\n",
    "plt.ylabel(plotLabels.stress)\n",
    "plt.legend();\n",
    "save_fig('compare-raw-data-rolling-mean',PATH_TO_FIGURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HBdUDsZ2f0T"
   },
   "source": [
    "## Fitting the raw data\n",
    "A non-linear least square method is used to fit the analytical model, ``get_stress_from_model``, to the given stress-strain curve. The optimization routine is implemented in the scientific computing libary Scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRU_TeruFEAb"
   },
   "outputs": [],
   "source": [
    "def get_stress_from_model(eps, C, n):\n",
    "    \"\"\"\n",
    "    Implementation of the desired \n",
    "    non-linear stress-strain relation\n",
    "    \"\"\"\n",
    "    return C*eps**(1.0/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFEMbQZF5j4v"
   },
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(get_stress_from_model, df_stress_strain_curve[\"eps\"], df_stress_strain_curve[\"sig\"], p0=[5,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rewXtVU77hK_"
   },
   "source": [
    "``pcov`` denotes the estimated covariance of the optimized parameters, ``popt``, while the elements on the diagonal provide the variances. To compute the standard deviation of the parameters, ``perr``, use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3j0XfyEj5ljH"
   },
   "outputs": [],
   "source": [
    "perr = np.sqrt(np.diag(pcov))\n",
    "C_fit, n_fit = popt\n",
    "C_fit_err, n_fit_err = perr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9TYskmMjL_x"
   },
   "outputs": [],
   "source": [
    "curve_fit_parameters = {'C': C_fit,\n",
    "                       'n': n_fit,\n",
    "                       'label': 'least squares fit ($C = {:.2f} \\pm {:.2f}$, $n = {:.2f} \\pm {:.2f}$)'.format(C_fit, C_fit_err, n_fit, n_fit_err),\n",
    "                       'save_name': \"compare-data-to-least-square-fit\"}                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmN680iBh-Bq"
   },
   "outputs": [],
   "source": [
    "def plot_raw_data_and_analytic_model(**kwargs):\n",
    "    fig, ax = plotStyle.create_figure()\n",
    "    plt.plot(\"eps\", \"sig\", data=df_stress_strain_curve, label='raw data', marker='o', linewidth=0)\n",
    "    plt.plot(df_stress_strain_curve[\"eps\"], get_stress_from_model(df_stress_strain_curve[\"eps\"], C=kwargs['C'], n=kwargs['n']), label=kwargs['label'])\n",
    "    plt.legend()\n",
    "    plt.xlabel(plotLabels.strain)\n",
    "    plt.ylabel(plotLabels.stress)\n",
    "    if kwargs['save_name'] != '':\n",
    "        save_fig(kwargs['save_name'], PATH_TO_FIGURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "EzlJ3ABNjhne",
    "outputId": "237323bc-4d3d-449b-850a-1a2ea732bb30"
   },
   "outputs": [],
   "source": [
    "plot_raw_data_and_analytic_model(**curve_fit_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brt_Jd0Jv-hn"
   },
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHXpnu8SvOzH"
   },
   "source": [
    "A data generator is implemented to obtain stress-strain curves. To this end, the analytical model (Ramberg-Osgood model) is used to produce stress values corresponding to the input values, namely, a given strain vector as well as the model parameters $C$ and $n$. In addition, Gaussian distributed noise is added to the analytically computed stress, while the standard deviation of the randomly added noise is chosen in accordance to the exemplary stress-strain curve, i.e. the average rolling standard deviation ``std_estimated_from_stress_strain_curve``. \n",
    "Each training data sample contains the strain and corresponding stress vectors as features, while the parameters for the generation, i.e. $C$ and $n$, are saved as targets (labels). \n",
    "The data generator is finally used to generate samples for various $C$ and $n$ value pairs. The $C$ and $n$ values are picked from uniform distributions in the range of $[C_{min}, C_{max}]$ and $[n_{min}, n_{max}]$, respectively. The number of datapoints is contant above all generated samples, however, the entries for the input vector $\\varepsilon$ are randomly distributed (uniform distribution) between a minimum strain $\\varepsilon_{min}$ and a maximum strain $\\varepsilon_{max}$. As a results, training a model with such randomized data should lead to an enhanced generalization performance and therefore a flexible use of the final model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXwLpUslMB_h"
   },
   "outputs": [],
   "source": [
    "class StressStrainDataGenerator(object):\n",
    "    def __init__(self, num_datapoints=101, eps_min=0.0, eps_max=1.0):\n",
    "        self.num_datapoints = num_datapoints\n",
    "        self.eps_min = eps_min \n",
    "        self.eps_max = eps_max\n",
    "        self.std_noise = 0.0\n",
    "\n",
    "    def get_stress_strain(self, C, n, add_noise=False, eps_linear_distributed=True):\n",
    "        if eps_linear_distributed:\n",
    "            eps = np.linspace(self.eps_min, self.eps_max, self.num_datapoints)\n",
    "        else:\n",
    "            eps = np.random.uniform(self.eps_min, self.eps_max, self.num_datapoints)\n",
    "            eps = np.sort(eps)\n",
    "        sig = get_stress_from_model(eps, C, n)\n",
    "        if add_noise:\n",
    "            sig += self.get_noise(len(sig), self.std_noise)       \n",
    "        return np.array([eps, sig]) \n",
    "\n",
    "    def shuffle_stress_strain_datasets(self, data, labels):\n",
    "        shuffle_index = np.random.permutation(len(labels))\n",
    "        return data[shuffle_index], labels[shuffle_index]\n",
    "\n",
    "    def get_noise(self, num_elements, sigma):\n",
    "        noise = np.random.normal(0, sigma, num_elements)\n",
    "        return noise        \n",
    "\n",
    "    def generate_stress_strain_datasets(self, num_samples=100, C_range=[5,15], n_range=[2,5], shuffle=True, add_noise=True, eps_linear_distributed=False):\n",
    "        generated_stress_strain_list = []\n",
    "        generated_parameters_list = []\n",
    "        for index_sample in range(num_samples):\n",
    "                C = np.random.uniform(C_range[0],C_range[1])\n",
    "                n = np.random.uniform(n_range[0],n_range[1])\n",
    "                generated_stress_strain = self.get_stress_strain(C, n, add_noise, eps_linear_distributed)\n",
    "                generated_stress_strain_list.append(generated_stress_strain)\n",
    "                generated_parameters_list.append(np.array([C, n]))\n",
    "        data = np.array(generated_stress_strain_list)\n",
    "        targets = np.array(generated_parameters_list) \n",
    "        if shuffle:\n",
    "            return self.shuffle_stress_strain_datasets(data, targets)  \n",
    "        else:            \n",
    "            return data, targets  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXm1lQ5BwSnE"
   },
   "outputs": [],
   "source": [
    "# The number of datapoints corresponds to the data set, which needs to be predicted.\n",
    "stressStrainDataGenerator = StressStrainDataGenerator(num_datapoints=NUM_DATAPOINTS) \n",
    "stressStrainDataGenerator.std_noise =  std_estimated_from_stress_strain_curve \n",
    "data, targets = stressStrainDataGenerator.generate_stress_strain_datasets(num_samples=2000,\n",
    "                                                                          C_range=[5,15],\n",
    "                                                                          n_range=[2,5],\n",
    "                                                                          shuffle=True,\n",
    "                                                                          add_noise=True,\n",
    "                                                                          eps_linear_distributed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79-fbKtlm8WG"
   },
   "source": [
    "Now some of the generated stress-strain curves can be plotted. Note that the amount of plotted curves is restricted here to only a very few. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "nAi7-n0XOkBV",
    "outputId": "e1ebd98c-1ad3-4570-a9fd-d9d7999358c0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plotStyle.create_figure()\n",
    "num_plotted_curves = 6\n",
    "for sample_index in range(0, num_plotted_curves):\n",
    "    x = data[sample_index][0]\n",
    "    y = data[sample_index][1]\n",
    "    label = '$C = {:.2f}$, $n = {:.2f}$'.format(*targets[sample_index])\n",
    "    plt.plot(x, y, label=label)\n",
    "plt.xlabel(plotLabels.strain)\n",
    "plt.ylabel(plotLabels.stress)\n",
    "plt.legend(ncol=2);\n",
    "save_fig(\"sample-generated-curves\", PATH_TO_FIGURES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWkzs9Obm9YC"
   },
   "source": [
    "# Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUc0OGrsCxPR"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hngmerW4iWDq"
   },
   "source": [
    "With the generated data it is very easy to split it to training and testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMrSraXzFzGC"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIUZ__FlkiDw"
   },
   "source": [
    "The generated training data do not have the right shape for the further use in the machine learning models. To this end, the ``DataTransformer`` is intoduced to reshape the data into the compatible shape, i.e. ``(num_samples, num_variables*num_datapoints)``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OcsisoiGk_2"
   },
   "outputs": [],
   "source": [
    "class DataTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        num_samples, num_variables, datapoints = X.shape\n",
    "        X = X.reshape((num_samples, num_variables * datapoints))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "857tTZJBjisa"
   },
   "source": [
    "Before the data is passed to a specific estimator, data transformations are carried out. First, the data is transformed using the ``DataTransformer``, while in a second step, the reshaped data is passed to a scaling function. For convenience, both steps are combined by defining apropriate pipelines. Finally, the data transformation pipeline is merged into a new pipeline, where the preprocessed data is now given to the specified estimator. \n",
    "Here, two distinct numeric transformers are defined to compare their performance during the hyperparameter optimization. \n",
    "\n",
    "For further information on pipelines in combination with hyperparameter optimization, check out the following project: \n",
    "https://www.kaggle.com/pedrodematos/titanic-a-complete-approach-to-top-6-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWytXz8eI-s_"
   },
   "outputs": [],
   "source": [
    "numeric_transformer_1 = Pipeline(steps=[('transform', DataTransformer()),\n",
    "                                        ('scaler', MinMaxScaler())])\n",
    "\n",
    "numeric_transformer_2 = Pipeline(steps=[('transform', DataTransformer()),\n",
    "                                        ('scaler', StandardScaler())])\n",
    "\n",
    "# Initializing data transformation step by choosing any of the above.\n",
    "# Initializing modeling step of the pipeline with any model object.\n",
    "pipe = Pipeline(steps=[('numeric_transformations', numeric_transformer_1), \n",
    "                       ('reg', RandomForestRegressor())]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1kQ7dzyzrat"
   },
   "source": [
    "## Train models and optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2eUfGcxrNaW"
   },
   "source": [
    "In the following, a predefined amount of hyperparameters are optimized through the Scikit-Learn class ``RandomSearchCV``. The parameter space is given by the following set:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yrlIR7yKBjh"
   },
   "outputs": [],
   "source": [
    "# grid search parameters:\n",
    "params_distributions = [{'numeric_transformations': [numeric_transformer_1, numeric_transformer_2],\n",
    "                         'reg': [RandomForestRegressor()],\n",
    "                         'reg__n_estimators': stats.randint(50, 300),\n",
    "                         'reg__criterion': ['mse']},\n",
    "               \n",
    "                        {'numeric_transformations': [numeric_transformer_1, numeric_transformer_2],\n",
    "                         'reg': [MultiOutputRegressor(SVR())],\n",
    "                         'reg__estimator__kernel': ['linear','rbf'],\n",
    "                         'reg__estimator__C': stats.uniform(loc=1, scale=9)}]              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HC1EXyPOBEe"
   },
   "outputs": [],
   "source": [
    "best_model_pipeline = RandomizedSearchCV(pipe, params_distributions, n_iter=5, \n",
    "                                         n_jobs=1, cv=3, random_state=42,\n",
    "                                         scoring='neg_mean_squared_error',\n",
    "                                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iak7XnrwtCWA"
   },
   "source": [
    "Finally, the estimators are fitted to the training data, while the hyperparameters are optimized through the random search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "Pnww1u4FOB5t",
    "outputId": "6f4a89b9-56d5-44d1-f559-64f8f4931f97"
   },
   "outputs": [],
   "source": [
    "best_model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R555m_EnyI64"
   },
   "source": [
    "The results of the best model found by the random search read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "aQO8cZeLd6qz",
    "outputId": "a31ef62a-ec06-4df2-a1d2-f306248aa5f7"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n",
    "print(\"\\n\\n#---------------- Best Regressor found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n",
    "print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZDhnjEIzb8p"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGk4e0ZazDSC"
   },
   "source": [
    "The Root-Mean-Square-Error (RMSE) between predicted and actual targets (labels) is in the following calculated on the train and test data. Note that the model did not seen the test data in the training process. Since the magnitude of the errors are similiar, the trained model shows a good generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Fbki_ZCzzDSL",
    "outputId": "52aa9cdc-adfa-46b0-96c8-e14e100004a8"
   },
   "outputs": [],
   "source": [
    "rmse_ml_train = np.sqrt(mean_squared_error(best_model_pipeline.predict(X_train), y_train))\n",
    "rmse_ml_test = np.sqrt(mean_squared_error(best_model_pipeline.predict(X_test), y_test))\n",
    "print('RMSE on train data = ',rmse_ml_train)\n",
    "print('RMSE on test data = ',rmse_ml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-I8Thyl0nI8"
   },
   "source": [
    "Finally, let's predict the model parameters related to the stress-strain curve, which has been analyzed in the beginning. In order to predict the parameters, the data needs to be reshaped according to expected input shape of the pipeline, i.e. ``(1, NUM_VARS, NUM_DATAPOINTS)``. Note that the batch size is here equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKq2bTA_Lcbr"
   },
   "outputs": [],
   "source": [
    "def transform_stress_strain_curve(df=df_stress_strain_curve):\n",
    "    stress_strain_curve_transformed = df_stress_strain_curve[[\"eps\", \"sig\"]].values.transpose()\n",
    "    stress_strain_curve_transformed = np.expand_dims(stress_strain_curve_transformed, axis=0)\n",
    "    return stress_strain_curve_transformed\n",
    "\n",
    "stress_strain_curve_transformed = transform_stress_strain_curve(df_stress_strain_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "OCnRjsQFgG2M",
    "outputId": "c00c6467-337e-4968-f81f-8849a34a231f"
   },
   "outputs": [],
   "source": [
    "C_pred, n_pred = best_model_pipeline.predict(stress_strain_curve_transformed)[0]\n",
    "\n",
    "best_model_parameters = {\"C\": C_pred,\n",
    "                         \"n\": n_pred,\n",
    "                         \"label\": \"prediction from best regressor ($C = {:.2f}$, $n = {:.2f}$)\".format(C_pred, n_pred),\n",
    "                         \"save_name\": \"compare-raw-data-ml-regressor\"}\n",
    "                         \n",
    "plot_raw_data_and_analytic_model(**best_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8W2XkxlfP3Q"
   },
   "source": [
    "# Neural Network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gPNHqnGVfyDB",
    "outputId": "29fea87d-0f26-4578-cd11-ec45d7d26ea5"
   },
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Sa186seZdNdY",
    "outputId": "7b86d1f9-4c6f-4d89-eafa-8354c5003bea"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.callbacks\n",
    "#import tensorflow_docs as tfdocs\n",
    "#import tensorflow_docs.plots\n",
    "#import tensorflow_docs.modeling\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyGImcuRgFdS"
   },
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpgLl3k0wk10"
   },
   "source": [
    "A flexible function to create a Neural Network (NN) with two hidden layers is presented in the following. The number of neurons per hidden layer can be adjusted among other parameters, e.g. the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIfKSG1kf03m"
   },
   "outputs": [],
   "source": [
    "def build_keras_model(num_input=202, num_output=2, act='relu', learning_rate=0.001, dropout_rate=0.2, hidden_layer_size_1=50, hidden_layer_size_2=25):  \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dropout(dropout_rate, input_shape=(num_input,)))\n",
    "    model.add(layers.Dense(hidden_layer_size_1, activation=act))\n",
    "    model.add(layers.Dense(hidden_layer_size_2, activation=act))\n",
    "    model.add(layers.Dense(num_output))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1ZIJcFNEPFe"
   },
   "source": [
    "In the following, a predefined amount of hyperparameters are optimized through Scikit-Learn class ``GridSearchCV``. To this end, the Keras model needs to be wrapped as a Scikit-Learn regressor through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4VrZzFRuPw5"
   },
   "outputs": [],
   "source": [
    "# pass in fixed parameters num_input and num_output\n",
    "NN_reg = KerasRegressor(build_fn = build_keras_model,\n",
    "                        num_input = NUM_DATAPOINTS*NUM_VARS,\n",
    "                        num_output = NUM_VARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYovt0I23ai6"
   },
   "source": [
    "## Train Neural Network and optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKjk_Bq21xDq"
   },
   "source": [
    "Constant fit parameters, ``fit_params``, for the training process are defined, while the grid search parameters are specified through ``params_grid``. The grid search is performed across all combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsD_GvZewfvV"
   },
   "outputs": [],
   "source": [
    "# Set fixed parameters for the training process.\n",
    "# Note these parameters could also be treated as \n",
    "# variable hyperparameters in the grid search.\n",
    "\n",
    "fit_params = {#'NN__callbacks': [tfdocs.modeling.EpochDots()],\n",
    "              'NN__batch_size': 100,\n",
    "              'NN__verbose': 0,\n",
    "              'NN__validation_split': 0.2}\n",
    "\n",
    "\n",
    "# grid search parameters:\n",
    "\n",
    "params_grid = [{'num_transformations': [numeric_transformer_2],\n",
    "               'NN': [NN_reg],\n",
    "               'NN__epochs': [500, 1000],\n",
    "               'NN__hidden_layer_size_1': [50, 100],\n",
    "               'NN__hidden_layer_size_2': [50, 100]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evCCt7F6288p"
   },
   "source": [
    "For convenience, the data transformation and the estimator are combined in a pipeline. Finally the grid search is initialized and fitted to the generated training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JJ57yvXvv01A",
    "outputId": "59726fbc-eced-41af-e996-c30b6f348bb8"
   },
   "outputs": [],
   "source": [
    "NN_pipe = Pipeline(steps=[('num_transformations', numeric_transformer_2), \n",
    "                          ('NN', NN_reg)])\n",
    "\n",
    "best_NN_pipeline = GridSearchCV(NN_pipe, \n",
    "                                param_grid = params_grid,\n",
    "                                scoring='neg_mean_squared_error', \n",
    "                                cv=3,\n",
    "                                n_jobs=1)\n",
    "\n",
    "best_NN_pipeline.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT0swePs4C2D"
   },
   "source": [
    "The results of the best NN found by the grid search read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "LLgmX_tCiDWv",
    "outputId": "31a3de25-5a10-4174-f2ce-483421b426c5"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n#---------------- Best Data Pipeline found in GridSearchCV  ----------------#\\n\\n\", best_NN_pipeline.best_estimator_[0])\n",
    "print(\"\\n\\n#---------------- Best Neural Network found in GridSearchCV  ----------------#\\n\\n\")\n",
    "print('Parameters:')\n",
    "for param, value in best_NN_pipeline.best_params_.items():\n",
    "    print('\\t{}: {}'.format(param, value))\n",
    "print(\"\\n\\n#---------------- Best Estimator's average score on CV (validation set) ----------------#\\n\\n\", best_NN_pipeline.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKYC1Pwt3-kD"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyemb0Ou40G6"
   },
   "source": [
    "Again, the RMSEs are computed on the training and test data. In addition to a small RMSE on the training data, a similar error on the test data indicates a good generalization performance of the trained model. \n",
    "\n",
    "Finally, the model parameters for the stress-strain curve analyzed in the beginning are predicted with the Neural Network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ZYxV3WN23d73",
    "outputId": "3272ac26-3175-4ad7-a2d5-ea01e7d774e3"
   },
   "outputs": [],
   "source": [
    "rmse_NN_train = np.sqrt(mean_squared_error(best_NN_pipeline.predict(X_train), y_train))\n",
    "rmse_NN_test = np.sqrt(mean_squared_error(best_NN_pipeline.predict(X_test), y_test))\n",
    "print('RMSE on train data = ', rmse_NN_train)\n",
    "print('RMSE on test data = ', rmse_NN_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "vOHSfAi9FBMa",
    "outputId": "13c1d7d5-07af-4acd-c201-d413e4c8f0b1"
   },
   "outputs": [],
   "source": [
    "C_pred_NN, n_pred_NN = best_NN_pipeline.predict(stress_strain_curve_transformed)\n",
    "estimated_parameters_NN = {\"C\": C_pred_NN,\n",
    "                           \"n\": n_pred_NN,\n",
    "                           \"label\": \"prediction NN ($C = {:.2f}$, $n = {:.2f}$)\".format(C_pred_NN, n_pred_NN),\n",
    "                           \"save_name\": \"compare-raw-data-NN-prediction\"}\n",
    "\n",
    "plot_raw_data_and_analytic_model(**estimated_parameters_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ2TNIIeS55S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model-calibration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
